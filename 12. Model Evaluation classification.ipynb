{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903938f7-7bd8-42b4-863b-081cbed15f65",
   "metadata": {},
   "source": [
    "# ‚úÖ Model Evaluation in Machine Learning\n",
    "\n",
    "## üéØ Why Evaluate a Model?\n",
    "\n",
    "Model evaluation helps assess how well your trained model generalizes to **unseen data**. It ensures:\n",
    "- The model is not overfitting or underfitting\n",
    "- The predictions are reliable\n",
    "- You can compare multiple models effectively\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Theoretical Overview\n",
    "\n",
    "Evaluation depends on the **type of problem**:\n",
    "- **Classification** (predicting categories)\n",
    "- **Regression** (predicting continuous values)\n",
    "\n",
    "Each type uses different metrics to assess performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Evaluation Metrics for Classification\n",
    "\n",
    "### 1. Accuracy\n",
    "\n",
    "- Good for balanced datasets\n",
    "- Misleading for imbalanced datasets\n",
    "\n",
    "### 2. Precision\n",
    "\n",
    "- How many of the predicted positives were actually correct?\n",
    "\n",
    "### 3. Recall (Sensitivity)\n",
    "\n",
    "- How many of the actual positives were captured?\n",
    "\n",
    "### 4. F1 Score\n",
    "\n",
    "-  Harmonic mean of Precision and Recall\n",
    "\n",
    "\n",
    "\n",
    "### 5. Confusion Matrix\n",
    "\n",
    "|               | Predicted Positive | Predicted Negative |\n",
    "|---------------|--------------------|--------------------|\n",
    "| Actual Positive | True Positive (TP) | False Negative (FN) |\n",
    "| Actual Negative | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "---\n",
    "\n",
    "### üêç Python Example (Classification)\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "```\n",
    "---\n",
    "## üìà Evaluation Metrics for Regression\n",
    "\n",
    "1. Mean Absolute Error (MAE)\n",
    "\n",
    "MAE = (1/n) * Œ£ |y·µ¢ - ≈∑·µ¢|\n",
    "\n",
    "- Measures average magnitude of errors\n",
    "\n",
    "2. Mean Squared Error (MSE)\n",
    "\n",
    "MSE = (1/n) * Œ£ (y·µ¢ - ≈∑·µ¢)¬≤\n",
    "\n",
    "- Penalizes larger errors more than MAE\n",
    "\n",
    "3. Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE = ‚àöMSE\n",
    "\n",
    "- More interpretable (same units as output variable)\n",
    "\n",
    "4. R¬≤ Score (Coefficient of Determination)\n",
    "\n",
    "R¬≤ = 1 - (SS_res / SS_tot)\n",
    "\n",
    "- Indicates how much variance is explained by the model (closer to 1 is better)\n",
    "\n",
    "üêç Python Example (Regression)\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598441d-1871-4a06-8ca5-f03d6e05c46b",
   "metadata": {},
   "source": [
    "| Concept      | Train Accuracy | Test Accuracy | Reason                        |\n",
    "| ------------ | -------------- | ------------- | ----------------------------- |\n",
    "| Overfitting  | High           | Low           | Model memorized training data |\n",
    "| Underfitting | Low            | Low           | Model too simple              |\n",
    "| Good Fit     | High           | High          | Model generalized well        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc95589-be76-4c8b-855b-1340a75c9b9c",
   "metadata": {},
   "source": [
    "# üìä Classification Metrics in Machine Learning\n",
    "\n",
    "When evaluating classification models, especially binary classifiers, these are the most commonly used metrics:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Accuracy\n",
    "\n",
    "**Definition:**\n",
    "The ratio of correctly predicted observations to the total observations.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "\n",
    "- TP = True Positives  \n",
    "- TN = True Negatives  \n",
    "- FP = False Positives  \n",
    "- FN = False Negatives\n",
    "\n",
    "**Use case:** Good when classes are balanced.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 2. Precision\n",
    "\n",
    "**Definition:**\n",
    "The ratio of correctly predicted positive observations to the total predicted positive observations.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "\n",
    "**Use case:** Important when **false positives** are costly (e.g., spam detection).\n",
    "\n",
    "---\n",
    "\n",
    "## üì¢ 3. Recall (Sensitivity or True Positive Rate)\n",
    "\n",
    "**Definition:**\n",
    "The ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "\n",
    "**Use case:** Important when **false negatives** are costly (e.g., disease diagnosis).\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ 4. F1 Score\n",
    "\n",
    "**Definition:**\n",
    "The harmonic mean of Precision and Recall. It balances the two metrics.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "\n",
    "**Use case:** Useful when you need a balance between Precision and Recall, especially with imbalanced classes.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò 5. Confusion Matrix\n",
    "\n",
    "**Definition:**\n",
    "A table used to evaluate the performance of a classification algorithm.\n",
    "\n",
    "### Structure:\n",
    "\n",
    "|                      | Predicted Positive | Predicted Negative |\n",
    "|----------------------|--------------------|--------------------|\n",
    "| **Actual Positive**  | True Positive (TP) | False Negative (FN)|\n",
    "| **Actual Negative**  | False Positive (FP)| True Negative (TN) |\n",
    "\n",
    "**Explanation:**\n",
    "- **TP**: Model correctly predicted the positive class\n",
    "- **TN**: Model correctly predicted the negative class\n",
    "- **FP**: Model incorrectly predicted positive (Type I Error)\n",
    "- **FN**: Model incorrectly predicted negative (Type II Error)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d82371-22c4-4cec-baa7-817faaadc9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
