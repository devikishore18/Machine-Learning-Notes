{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e43d2c9-8d85-40e2-9513-0dd128d229f9",
   "metadata": {},
   "source": [
    "# 📘 Supervised Machine Learning Algorithms with Theory & Math\n",
    "\n",
    "Supervised learning is a type of machine learning where the model is trained on **labeled data** — meaning each input has a corresponding output. The model learns to map inputs to outputs and can make predictions on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Table of Contents\n",
    "1. [Linear Regression](#1-linear-regression)\n",
    "2. [Logistic Regression](#2-logistic-regression)\n",
    "3. [K-Nearest Neighbors (KNN)](#3-k-nearest-neighbors-knn)\n",
    "4. [Support Vector Machines (SVM)](#4-support-vector-machines-svm)\n",
    "5. [Decision Trees](#5-decision-trees)\n",
    "6. [Random Forest](#6-random-forest)\n",
    "7. [Gradient Boosting (XGBoost, LightGBM)](#7-gradient-boosting)\n",
    "8. [Naive Bayes](#8-naive-bayes)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Linear Regression\n",
    "\n",
    "### 📌 Definition:\n",
    "Linear Regression is a statistical method used to **predict a continuous output** based on one or more input features by fitting a linear relationship.\n",
    "\n",
    "### 📖 Theoretical Intuition:\n",
    "- Assumes a **linear relationship** between the independent variables and the dependent variable.\n",
    "- Estimates coefficients (weights) to minimize prediction error.\n",
    "\n",
    "### 📐 Equation:\n",
    "\\[\n",
    "\\hat{y} = w_0 + w_1x_1 + w_2x_2 + \\dots + w_nx_n\n",
    "\\]\n",
    "\n",
    "### 🎯 Cost Function (MSE):\n",
    "\\[\n",
    "J(w) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n",
    "\\]\n",
    "\n",
    "### ⚙ Optimization:\n",
    "- Uses **Gradient Descent** or **Normal Equation** to minimize error.\n",
    "\n",
    "### ✅ Use Cases:\n",
    "- Predicting prices (housing, stock)\n",
    "- Sales forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Logistic Regression\n",
    "\n",
    "### 📌 Definition:\n",
    "Logistic Regression is used for **binary classification** problems. It estimates the probability that a given input belongs to a certain class.\n",
    "\n",
    "### 📖 Theoretical Intuition:\n",
    "- Applies the **sigmoid function** to output a probability between 0 and 1.\n",
    "- Uses **cross-entropy** as a loss function.\n",
    "\n",
    "### 📐 Hypothesis:\n",
    "\\[\n",
    "\\hat{y} = \\sigma(w^T x) = \\frac{1}{1 + e^{-w^T x}}\n",
    "\\]\n",
    "\n",
    "### 🎯 Loss Function:\n",
    "\\[\n",
    "J(w) = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right]\n",
    "\\]\n",
    "\n",
    "### ⚙ Optimization:\n",
    "- Gradient Descent, L-BFGS\n",
    "\n",
    "### ✅ Use Cases:\n",
    "- Email spam detection\n",
    "- Disease diagnosis (yes/no)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. K-Nearest Neighbors (KNN)\n",
    "\n",
    "### 📌 Definition:\n",
    "KNN is a **non-parametric**, instance-based algorithm that classifies new instances based on the majority class of its **k closest neighbors**.\n",
    "\n",
    "### 📖 Theoretical Intuition:\n",
    "- Stores the entire training dataset.\n",
    "- Makes predictions by calculating the **distance** to all training points.\n",
    "\n",
    "### 📐 Distance Metric (Euclidean):\n",
    "\\[\n",
    "d(x, x_i) = \\sqrt{\\sum_{j=1}^n (x_j - x_{ij})^2}\n",
    "\\]\n",
    "\n",
    "### ✅ Use Cases:\n",
    "- Recommender systems\n",
    "- Handwriting recognition\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Support Vector Machines (SVM)\n",
    "\n",
    "### 📌 Definition:\n",
    "SVM is a **maximum margin classifier** that finds the optimal hyperplane to separate different classes in the feature space.\n",
    "\n",
    "### 📖 Theoretical Intuition:\n",
    "- Maximizes the distance (margin) between the nearest data points and the decision boundary.\n",
    "- Can handle **non-linear data** using the **kernel trick**.\n",
    "\n",
    "### 📐 Objective Function:\n",
    "\\[\n",
    "\\text{maximize } \\frac{2}{\\|w\\|} \\quad \\text{subject to } y_i(w^T x_i + b) \\geq 1\n",
    "\\]\n",
    "\n",
    "### 🎯 Hinge Loss:\n",
    "\\[\n",
    "J(w) = \\frac{1}{2}\\|w\\|^2 + C \\sum_{i=1}^n \\max(0, 1 - y_i(w^T x_i + b))\n",
    "\\]\n",
    "\n",
    "### ✅ Use Cases:\n",
    "- Image classification\n",
    "- Bioinformatics\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Decision Trees\n",
    "\n",
    "### 📌 Definition:\n",
    "A Decision Tree is a **flowchart-like** structure where internal nodes represent feature tests, branches represent outcomes, and leaf nodes represent decisions.\n",
    "\n",
    "### 📖 Theoretical Intuition:\n",
    "- Splits data on features that provide the **highest information gain** (or lowest impurity).\n",
    "- Easy to interpret and visualize.\n",
    "\n",
    "### 📐 Gini Index:\n",
    "\\[\n",
    "G = 1 - \\sum_{i=1}^C p_i^2\n",
    "\\]\n",
    "\n",
    "### 📐 Entropy:\n",
    "\\[\n",
    "H = - \\sum_{i=1}^C p_i \\log_2(p_i)\n",
    "\\]\n",
    "\n",
    "### ✅ Use Cases:\n",
    "- Customer segmentation\n",
    "- Fraud detection\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Random Forest\n",
    "\n",
    "### 📌 Definition:\n",
    "Random Forest is an **ensemble learning method** that builds multiple decision trees and merges their results for better accuracy.\n",
    "\n",
    "### 📖 Theoretical Intuition:\n",
    "- Uses **Bagging** (bootstrap aggregating) to reduce variance.\n",
    "- At each split, only a **random subset of features** is considered.\n",
    "\n",
    "### 🧠 Prediction:\n",
    "- Classification: **Majority vote**\n",
    "- Regression: **Mean of predictions**\n",
    "\n",
    "### ✅ Use Cases:\n",
    "- Loan approval prediction\n",
    "- Credit scoring\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Gradient Boosting\n",
    "\n",
    "### 📌 Definition:\n",
    "Gradient Boosting is an ensemble method that builds models **sequentially**, each correcting the errors of its predecessor.\n",
    "\n",
    "### 📖 Theoretical Intuition:\n",
    "- Models the **residual errors** from previous models.\n",
    "- Each learner tries to reduce the **gradient** of the loss function.\n",
    "\n",
    "### 📐 Model Update:\n",
    "\\[\n",
    "F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( F_m \\): current model\n",
    "- \\( \\eta \\): learning rate\n",
    "- \\( h_m(x) \\): new weak learner\n",
    "\n",
    "### ✅ Use Cases:\n",
    "- Ranking (search engines)\n",
    "- Customer churn prediction\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Naive Bayes\n",
    "\n",
    "### 📌 Definition:\n",
    "Naive Bayes is a **probabilistic classifier** based on **Bayes' Theorem**, assuming **independence between features**.\n",
    "\n",
    "### 📖 Theoretical Intuition:\n",
    "- Despite the independence assumption, performs well in many applications.\n",
    "- Very efficient and requires a small amount of training data.\n",
    "\n",
    "### 📐 Bayes’ Theorem:\n",
    "\\[\n",
    "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
    "\\]\n",
    "\n",
    "### 📐 Classification Rule:\n",
    "\\[\n",
    "\\hat{y} = \\arg\\max_C P(C) \\prod_{i=1}^n P(x_i | C)\n",
    "\\]\n",
    "\n",
    "### ✅ Use Cases:\n",
    "- Spam filtering\n",
    "- Sentiment analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Summary Table\n",
    "\n",
    "| Algorithm         | Type           | Key Concept                         | Math Element                     |\n",
    "|-------------------|----------------|-------------------------------------|----------------------------------|\n",
    "| Linear Regression | Regression     | Line of best fit                    | MSE + Gradient Descent          |\n",
    "| Logistic Regression | Classification | Probability Estimation            | Sigmoid + Cross-Entropy         |\n",
    "| KNN               | Both           | Nearest neighbors                   | Distance (Euclidean, etc.)      |\n",
    "| SVM               | Classification | Maximum Margin                      | Hinge Loss + Kernels            |\n",
    "| Decision Tree     | Both           | Feature splits                      | Gini/Entropy                    |\n",
    "| Random Forest     | Both           | Ensemble of trees                   | Bagging                         |\n",
    "| Gradient Boosting | Both           | Residual learning                   | Additive Models + Gradient      |\n",
    "| Naive Bayes       | Classification | Probabilistic model                 | Bayes’ Theorem + Product Rule   |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67b7bd-eee3-4108-9382-e1c488c9d7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
