{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef8ea21-fa0a-409f-9d88-fa54331ded43",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Unsupervised Machine Learning Algorithms with Theory & Math\n",
    "\n",
    "Unsupervised learning is a type of machine learning where **no labeled output is provided**. The model explores the structure of the data to identify patterns, groupings, or dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Table of Contents\n",
    "\n",
    "1. [K-Means Clustering](#1-k-means-clustering)\n",
    "2. [Hierarchical Clustering](#2-hierarchical-clustering)\n",
    "3. [Principal Component Analysis (PCA)](#3-principal-component-analysis-pca)\n",
    "4. [DBSCAN (Density-Based Spatial Clustering)](#4-dbscan)\n",
    "5. [t-SNE (t-Distributed Stochastic Neighbor Embedding)](#5-t-sne)\n",
    "6. [Autoencoders (Neural Nets)](#6-autoencoders)\n",
    "7. [Gaussian Mixture Models (GMM)](#7-gaussian-mixture-models)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. K-Means Clustering\n",
    "\n",
    "### ğŸ“Œ Definition:\n",
    "K-Means is a **centroid-based clustering algorithm** that partitions data into **K clusters**, minimizing the variance within each cluster.\n",
    "\n",
    "### ğŸ“– Theoretical Intuition:\n",
    "- Iteratively assigns data points to the nearest cluster centroid.\n",
    "- Recomputes centroids until convergence.\n",
    "\n",
    "### ğŸ“ Objective Function:\n",
    "\\[\n",
    "J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( C_i \\): Cluster i\n",
    "- \\( \\mu_i \\): Centroid of cluster i\n",
    "\n",
    "### âœ… Use Cases:\n",
    "- Customer segmentation\n",
    "- Image compression\n",
    "- Document clustering\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Hierarchical Clustering\n",
    "\n",
    "### ğŸ“Œ Definition:\n",
    "Builds a hierarchy of clusters using a **bottom-up (agglomerative)** or **top-down (divisive)** approach.\n",
    "\n",
    "### ğŸ“– Theoretical Intuition:\n",
    "- Agglomerative: Each data point starts as its own cluster and merges iteratively.\n",
    "- Dendrograms are used to visualize the cluster hierarchy.\n",
    "\n",
    "### ğŸ“ Linkage Criteria:\n",
    "- **Single linkage**: Minimum distance\n",
    "- **Complete linkage**: Maximum distance\n",
    "- **Average linkage**: Average distance\n",
    "\n",
    "### âœ… Use Cases:\n",
    "- Gene expression analysis\n",
    "- Social network analysis\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Principal Component Analysis (PCA)\n",
    "\n",
    "### ğŸ“Œ Definition:\n",
    "PCA is a **dimensionality reduction** technique that transforms data into a new set of axes (**principal components**) that maximize variance.\n",
    "\n",
    "### ğŸ“– Theoretical Intuition:\n",
    "- Orthogonal transformation\n",
    "- First principal component accounts for the most variance\n",
    "\n",
    "### ğŸ“ Math Formulation:\n",
    "- Given covariance matrix \\( \\Sigma \\), compute eigenvalues and eigenvectors\n",
    "- Transform:\n",
    "\\[\n",
    "Z = X \\cdot W\n",
    "\\]\n",
    "Where:\n",
    "- \\( W \\): matrix of top k eigenvectors\n",
    "\n",
    "### âœ… Use Cases:\n",
    "- Data compression\n",
    "- Noise reduction\n",
    "- Visualization (2D or 3D projection)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. DBSCAN\n",
    "\n",
    "### ğŸ“Œ Definition:\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups together points that are **close to each other** (dense regions) and labels others as noise.\n",
    "\n",
    "### ğŸ“– Theoretical Intuition:\n",
    "- Doesnâ€™t require specifying the number of clusters.\n",
    "- Detects arbitrary-shaped clusters and noise.\n",
    "\n",
    "### ğŸ“ Parameters:\n",
    "- \\( \\varepsilon \\): Radius\n",
    "- \\( \\text{MinPts} \\): Minimum number of points to form a dense region\n",
    "\n",
    "### âœ… Use Cases:\n",
    "- Outlier detection\n",
    "- Spatial data (e.g., geolocation)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. t-SNE\n",
    "\n",
    "### ğŸ“Œ Definition:\n",
    "t-SNE (t-distributed Stochastic Neighbor Embedding) is a **non-linear dimensionality reduction** technique that is primarily used for **visualizing high-dimensional data**.\n",
    "\n",
    "### ğŸ“– Theoretical Intuition:\n",
    "- Converts high-dimensional Euclidean distances into conditional probabilities.\n",
    "- Minimizes Kullback-Leibler divergence between two distributions.\n",
    "\n",
    "### ğŸ“ Optimization Goal:\n",
    "\\[\n",
    "\\text{KL}(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( p_{ij} \\): similarity in high dimension\n",
    "- \\( q_{ij} \\): similarity in low dimension\n",
    "\n",
    "### âœ… Use Cases:\n",
    "- Data visualization (e.g., MNIST digits)\n",
    "- Exploring data embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Autoencoders\n",
    "\n",
    "### ğŸ“Œ Definition:\n",
    "Autoencoders are **neural networks** that learn to compress and then reconstruct data, often used for **dimensionality reduction and anomaly detection**.\n",
    "\n",
    "### ğŸ“– Theoretical Intuition:\n",
    "- Encoder compresses input to latent space\n",
    "- Decoder reconstructs the input from compressed code\n",
    "\n",
    "### ğŸ“ Loss Function:\n",
    "\\[\n",
    "L = \\|X - \\hat{X}\\|^2\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( X \\): Original input\n",
    "- \\( \\hat{X} \\): Reconstructed input\n",
    "\n",
    "### âœ… Use Cases:\n",
    "- Denoising\n",
    "- Anomaly detection\n",
    "- Representation learning\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Gaussian Mixture Models (GMM)\n",
    "\n",
    "### ğŸ“Œ Definition:\n",
    "GMM is a **probabilistic model** assuming all data points are generated from a mixture of several Gaussian distributions.\n",
    "\n",
    "### ğŸ“– Theoretical Intuition:\n",
    "- Uses the **Expectation-Maximization (EM)** algorithm to estimate parameters.\n",
    "\n",
    "### ğŸ“ Probability Density:\n",
    "\\[\n",
    "P(x) = \\sum_{k=1}^{K} \\pi_k \\cdot \\mathcal{N}(x | \\mu_k, \\Sigma_k)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\pi_k \\): Mixing coefficient\n",
    "- \\( \\mu_k, \\Sigma_k \\): Mean and covariance of each component\n",
    "\n",
    "### âœ… Use Cases:\n",
    "- Speaker recognition\n",
    "- Clustering with soft boundaries\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Summary Table\n",
    "\n",
    "| Algorithm         | Purpose                | Key Concept                     | Mathematical Tool                   |\n",
    "|-------------------|------------------------|----------------------------------|--------------------------------------|\n",
    "| K-Means           | Clustering             | Minimize intra-cluster distance | Euclidean distance                  |\n",
    "| Hierarchical Clustering | Clustering       | Cluster tree structure          | Linkage + Dendrograms               |\n",
    "| PCA               | Dimensionality Reduction | Maximize variance              | Eigen decomposition of covariance   |\n",
    "| DBSCAN            | Clustering + Outlier Detection | Density-based clustering     | Epsilon + MinPts                     |\n",
    "| t-SNE             | Visualization          | Preserve local similarity       | KL Divergence                        |\n",
    "| Autoencoders      | Dimensionality Reduction | Reconstruction learning       | Neural Networks + MSE Loss          |\n",
    "| GMM               | Soft Clustering        | Mixture of Gaussians            | EM Algorithm + Probability Estimation|\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6c423-7fe0-4d15-8b6f-c635b66f25a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
