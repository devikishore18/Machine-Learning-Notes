{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72dd3f6-648e-4850-a697-eb53852906c4",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Model Tuning in Machine Learning\n",
    "\n",
    "Model tuning (also called **hyperparameter optimization**) is the process of improving a machine learning model‚Äôs performance by systematically choosing the best set of hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "## üéõÔ∏è What Are Hyperparameters?\n",
    "\n",
    "- **Hyperparameters** are external configurations **set before training** (e.g., learning rate, number of trees, max depth).\n",
    "- They are different from **parameters** (like weights), which are learned by the model during training.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why Is Tuning Important?\n",
    "\n",
    "- Prevents **underfitting** (model too simple) and **overfitting** (model too complex).\n",
    "- Helps achieve optimal performance on **unseen/test data**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Common Tuning Techniques\n",
    "\n",
    "### 1. **Grid Search**\n",
    "\n",
    "**Definition:** Exhaustively tries all possible combinations of hyperparameters from a specified grid.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'n_estimators': [100, 200], 'max_depth': [3, 5, 7]}\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid=params, cv=5)\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab641250-bfef-4409-9162-0838e0da74d2",
   "metadata": {},
   "source": [
    "- ‚úÖ Pros: Simple to implement\n",
    "- ‚ö†Ô∏è Cons: Can be computationally expensive\n",
    "\n",
    "## 2. Randomized Search\n",
    "Definition: Randomly samples a given number of combinations from a hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324c6d9-d3a1-4926-8816-d250a1ae0435",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "params = {'n_estimators': randint(100, 500), 'max_depth': randint(3, 10)}\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=params, n_iter=10, cv=5)\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b4e3a-22b4-447f-813f-1255d1799973",
   "metadata": {},
   "source": [
    "- ‚úÖ Pros: Faster than grid search\n",
    "- ‚ö†Ô∏è Cons: Might miss optimal parameters\n",
    "\n",
    "## 3. Bayesian Optimization (Advanced)\n",
    "Uses probabilistic models to estimate the best next set of parameters.\n",
    "\n",
    "Efficient for complex models or large hyperparameter spaces.\n",
    "\n",
    "Examples: Optuna, Hyperopt, Scikit-Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e69774-9432-4ff1-94a3-e037964401f7",
   "metadata": {},
   "source": [
    "| Algorithm         | Hyperparameters                                |\n",
    "| ----------------- | ---------------------------------------------- |\n",
    "| Linear Regression | alpha (in Ridge/Lasso), fit\\_intercept         |\n",
    "| Decision Trees    | max\\_depth, min\\_samples\\_split, criterion     |\n",
    "| Random Forest     | n\\_estimators, max\\_depth, max\\_features       |\n",
    "| SVM               | C, kernel, gamma                               |\n",
    "| XGBoost/LightGBM  | learning\\_rate, n\\_estimators, max\\_depth      |\n",
    "| KNN               | n\\_neighbors, weights, metric                  |\n",
    "| Neural Networks   | learning\\_rate, batch\\_size, epochs, optimizer |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b46a6-7c84-433d-a19e-7ed1ab35c411",
   "metadata": {},
   "source": [
    "## üß† Best Practices\n",
    "- Start with domain knowledge to narrow the search space.\n",
    "- Use cross-validation (e.g., k-fold) during tuning to avoid overfitting.\n",
    "- Monitor training time ‚Äî more tuning means more compute.\n",
    "- Use early stopping for iterative models like XGBoost or neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bf0b6-f7b3-4809-b552-fb6235e9e47d",
   "metadata": {},
   "source": [
    "üìå Tip: After tuning, always evaluate your model on a hold-out test set to measure final generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433d7bd-d64d-4893-8edf-5db2792e44b9",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Hyperparameter Tuning in Machine Learning\n",
    "\n",
    "Hyperparameter tuning is the process of **finding the best set of hyperparameters** for a machine learning model to improve its **accuracy and generalization** on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## üîë What Are Hyperparameters?\n",
    "\n",
    "- **Hyperparameters** are values set **before training** begins.\n",
    "- Unlike model **parameters** (like weights), hyperparameters **control** the learning process.\n",
    "\n",
    "**Examples:**\n",
    "- `learning_rate` in gradient boosting\n",
    "- `C` and `kernel` in SVM\n",
    "- `max_depth`, `n_estimators` in tree-based models\n",
    "- `batch_size`, `epochs` in neural networks\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why Tune Hyperparameters?\n",
    "\n",
    "- Prevents **overfitting** or **underfitting**\n",
    "- Improves **model accuracy and robustness**\n",
    "- Optimizes model **efficiency and performance**\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Hyperparameter Tuning Techniques\n",
    "\n",
    "### üî∏ 1. Grid Search\n",
    "\n",
    "- Tests **all combinations** of a predefined hyperparameter grid.\n",
    "- Suitable for **small search spaces**.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': [3, 5], 'n_estimators': [100, 200]}\n",
    "grid = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97ca79-94f4-492d-ae4e-b66c93997f01",
   "metadata": {},
   "source": [
    "## üî∏ 2. Randomized Search\n",
    "Randomly samples combinations from a hyperparameter space.\n",
    "\n",
    "More efficient than grid search for large parameter spaces.\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {'n_estimators': randint(100, 500), 'max_depth': randint(3, 10)}\n",
    "random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist, n_iter=10, cv=5)\n",
    "random_search.fit(X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84765aa4-5479-4620-8181-9fad21b4cbe2",
   "metadata": {},
   "source": [
    "‚úÖ Less computation, faster\n",
    "‚ùå Might miss the optimal combination\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b198ca6-2f3f-4e61-b399-1be4346feedb",
   "metadata": {},
   "source": [
    "## üî∏ 3. Bayesian Optimization\n",
    "Uses probability models to choose promising hyperparameters.\n",
    "\n",
    "Efficient for complex and expensive models.\n",
    "\n",
    "Popular Libraries:\n",
    "\n",
    "Optuna,Hyperopt,Scikit-Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63d73b-0ae0-42e3-941a-a2554dd7e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
