{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a898e8a2-1eb6-403a-bc23-f59669d96454",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Training the Model in Machine Learning\n",
    "\n",
    "## üìò What Does \"Training a Model\" Mean?\n",
    "\n",
    "Training a model refers to the process where the machine learning algorithm **learns the patterns** and **relationships** in the data by adjusting internal parameters to minimize the **loss function**.\n",
    "\n",
    "In supervised learning, this involves:\n",
    "- Input features `X`\n",
    "- Target labels `y`\n",
    "- A hypothesis function `h(X) ‚âà y`\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Theoretical Understanding\n",
    "\n",
    "### üßÆ Objective Function\n",
    "\n",
    "The goal of training is to minimize the **loss function**:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ec4ba-c1db-4e6b-b072-fa716ebe887c",
   "metadata": {},
   "source": [
    "## üéØ Loss Function vs Cost Function\n",
    "\n",
    "In supervised learning, the model learns by **minimizing error** between actual and predicted outputs. This is done using:\n",
    "\n",
    "### üîπ Loss Function\n",
    "\n",
    "**Loss = Actual Value - Predicted Value**\n",
    "\n",
    "- Measures error for **a single observation**\n",
    "- Example (Mean Squared Error):\n",
    "  \n",
    "Or in a squared form (commonly used for regression):\n",
    "\n",
    "\n",
    "**Loss = (y - ≈∑)¬≤**\n",
    "\n",
    "- y = actual/true value\n",
    "- ≈∑ = predicted value\n",
    "\n",
    "Measures how far off a single prediction is from the actual result.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ  Cost Function\n",
    "Formula:\n",
    "\n",
    "```bash\n",
    "\n",
    "Cost = (1/m) * Œ£ [Loss for each training example]\n",
    "     = (1/m) * Œ£ (y·µ¢ - ≈∑·µ¢)¬≤\n",
    "```\n",
    "- m = number of training examples\n",
    "- y·µ¢ = actual value of the i-th data point\n",
    "- ≈∑·µ¢ = predicted value for the i-th data point\n",
    "\n",
    "The cost function is the average of the loss values over the entire dataset.\n",
    "\n",
    "The goal of training is to minimize this cost by adjusting model parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012f018-cc03-4ce6-807d-e33f809cbda1",
   "metadata": {},
   "source": [
    "### üîÅ Optimization Algorithm\n",
    "\n",
    "Most models use **Gradient Descent** to minimize the cost:\n",
    "\n",
    "üîΩ Gradient Descent Update Rule\n",
    "Formula:\n",
    "\n",
    "**Œ∏ = Œ∏ - Œ± * ‚àáJ(Œ∏)**\n",
    "\n",
    "Where:\n",
    "- Œ∏ = model parameters (like weights in linear regression)\n",
    "- Œ± = learning rate (a small constant that controls how big a step we take)\n",
    "- ‚àáJ(Œ∏) = gradient of the cost function with respect to parameters (i.e., the direction and rate of fastest increase in the cost)\n",
    "\n",
    "### üìò Explanation:\n",
    "The gradient ‚àáJ(Œ∏) tells us how the cost function changes with each parameter.\n",
    "\n",
    "We subtract the gradient because we want to move in the direction that reduces the cost (i.e., downhill).\n",
    "\n",
    "The learning rate Œ± controls how big a step we take in each iteration. If it‚Äôs too large, we may overshoot; if too small, convergence will be slow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce7b2f-e956-4522-b50c-58ecb116305d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Steps in Model Training\n",
    "\n",
    "1. **Select Algorithm**: e.g., Logistic Regression, Decision Tree\n",
    "2. **Initialize Model Parameters**\n",
    "3. **Feed Training Data**: Input features and target labels\n",
    "4. **Model Learns**: Internal weights are updated based on loss\n",
    "5. **Stop Criteria**: Based on convergence, iteration count, or early stopping\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Python Example: Training a Classifier\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Define the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Train the model on training data\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dc59a-b4d6-4ed0-bb6a-59e9c53178bc",
   "metadata": {},
   "source": [
    "## üß™ Checking Model Performance (Basic)\n",
    "```python\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "## üìå Key Points\n",
    "- Always train on training data, never on test data.\n",
    "- Preprocess the data before training (scaling, encoding, imputation).\n",
    "- Training may involve epochs and batches (especially in neural networks).\n",
    "- For neural networks: use .compile() and .fit() in Keras/TensorFlow.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4aad3d-b315-4c70-ad01-ca0dced4fea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
